{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicelindel3/nlp/blob/main/Section_07/distributed_expression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz7grWACvOQS"
      },
      "source": [
        "# 分散表現の確認\n",
        "word2vecによる分散表現について学びます。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip list"
      ],
      "metadata": {
        "id": "4MLliaXn5YuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install janome"
      ],
      "metadata": {
        "id": "5dtMTfFyvZiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SR3G8wVvOQZ"
      },
      "source": [
        "## コーパスの前処理\n",
        "前のセクションと同様に、コーパスに前処理を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k8CMfDVvOQa"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "from janome.tokenizer import Tokenizer\n",
        "\n",
        "with open(\"wagahaiwa_nekodearu.txt\", mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
        "    wagahai_original = f.read()\n",
        "\n",
        "wagahai = re.sub(\"《[^》]+》\", \"\", wagahai_original) # ルビの削除\n",
        "wagahai = re.sub(\"［[^］]+］\", \"\", wagahai) # 読みの注意の削除\n",
        "wagahai = re.sub(\"[｜ 　「」\\n]\", \"\", wagahai) # | と全角半角スペース、「」と改行の削除\n",
        "\n",
        "seperator = \"。\"  # 。をセパレータに指定\n",
        "wagahai_list = wagahai.split(seperator)  # セパレーターを使って文章をリストに分割する\n",
        "wagahai_list.pop() # 最後の要素は空の文字列になるので、削除\n",
        "wagahai_list = [x+seperator for x in wagahai_list]  # 文章の最後に。を追加\n",
        "        \n",
        "t = Tokenizer()\n",
        "\n",
        "wagahai_words = []\n",
        "for sentence in wagahai_list:\n",
        "    wagahai_words.append(list(t.tokenize(sentence, wakati=True)))  # 文章ごとに単語に分割し、リストに格納\n",
        "    \n",
        "with open('wagahai_words.pickle', mode='wb') as f:  # pickleに保存\n",
        "    pickle.dump(wagahai_words, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N52a2RZ7vOQd"
      },
      "source": [
        "保存できていることを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slj4Pat9vOQe"
      },
      "outputs": [],
      "source": [
        "with open('wagahai_words.pickle', mode='rb') as f:\n",
        "    wagahai_words = pickle.load(f)\n",
        "\n",
        "print(wagahai_words[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cBS8o4ZvOQf"
      },
      "source": [
        "## word2vecを用いた学習\n",
        "今回はword2vecのためにライブラリgensimを使います。  \n",
        "gensimは、様々なトピックモデルを実装したPythonライブラリです。  \n",
        "トピックモデルとは、潜在的なトピックから文章が確率的に生成されると仮定したモデルです。\n",
        "\n",
        "gensimについて、詳細は以下のリンクを参考にどうぞ。  \n",
        "https://radimrehurek.com/gensim/\n",
        "\n",
        "以下では、word2vecを用いてコーパスの学習を行い、学習済みのモデルを作成します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TIZx-_FvOQf"
      },
      "outputs": [],
      "source": [
        "from gensim.models import word2vec\n",
        "\n",
        "# size : 中間層のニューロン数\n",
        "# min_count : この値以下の出現回数の単語を無視\n",
        "# window : 対象単語を中心とした前後の単語数\n",
        "# iter : epochs数\n",
        "# sg : skip-gramを使うかどうか 0:CBOW 1:skip-gram\n",
        "model = word2vec.Word2Vec(wagahai_words,\n",
        "                          size=100, #version 3.6 -> 4.0 vector_size\n",
        "                          min_count=5,\n",
        "                          window=5,\n",
        "                          iter=20, #version 3.6 -> 4.0 epochs\n",
        "                          sg = 0)\n",
        "                          "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDoM8sSNvOQh"
      },
      "source": [
        "分散表現を見ていきましょう。  \n",
        "重みを表す行列（分散表現）の形状と、行列そのものを表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zk_65lavOQh"
      },
      "outputs": [],
      "source": [
        "print(model.wv.vectors.shape)  # 分散表現の形状\n",
        "print(model.wv.vectors)  # 分散表現"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxfhcST2vOQi"
      },
      "source": [
        "語彙の数、および語彙の最初の10語を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYqf3zayvOQj"
      },
      "outputs": [],
      "source": [
        "print(len(model.wv.index2word))  # 語彙の数 version 3.6 -> 4.0 model.wv.index_to_key\n",
        "print(model.wv.index2word[:10])  # 最初の10単語 version 3.6 -> 4.0 model.wv.index_to_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4C0uW3pvOQj"
      },
      "source": [
        "語彙における、最初の単語の単語ベクトルを2通りの方法で表示します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7sYZ1tSvOQk"
      },
      "outputs": [],
      "source": [
        "print(model.wv.vectors[0])  # 最初のベクトル\n",
        "print(model.wv.__getitem__(\"の\"))  # 最初の単語「の」のベクトル"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukQ00yyxvOQk"
      },
      "source": [
        "両者ともに同じベクトルですね。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtxE5BE8vOQk"
      },
      "source": [
        "## 課題:\n",
        "単語「猫」を単語ベクトルで表してみましょう。  \n",
        "また、skip-gramも試して結果を比較してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZsfLqj6vOQl"
      },
      "outputs": [],
      "source": [
        "print(model.wv.__getitem__(\"猫\")) "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "distributed_expression.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}