{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicelindel3/nlp/blob/main/Section_10/dialogue_corpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcqJPKwFJqCa"
      },
      "source": [
        "# 対話コーパスの前処理\n",
        "以降のレクチャーでは、Seq2Seqを使って対話文を生成します。  \n",
        "宮沢賢治の小説を学習データに使い、賢治風の返事ができるようにモデルを訓練します。  \n",
        "今回は、そのための準備として、コーパスに前処理を行います。  \n",
        "小規模なコーパスでも対話文が生成ができるように、漢字は全てひらがなに変換します。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "8h7qPlB8MdY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eC4NqGGJqCp"
      },
      "source": [
        "## テキストデータの前処理\n",
        "今回は、宮沢賢治の小説「銀河鉄道の夜」「セロ弾きのゴーシュ」「注文の多い料理店」などをコーパスとします。  \n",
        "コーパスが大きほど精度が高まるのですが、学習に時間がかかり過ぎても本コースに支障があるので、10作品に抑えておきます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XwJ9KHDhJqCr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "novels = [\"gingatetsudono_yoru.txt\", \"serohikino_goshu.txt\", \"chumonno_oi_ryoriten.txt\",\n",
        "         \"gusukobudorino_denki.txt\", \"kaeruno_gomugutsu.txt\", \"kaino_hi.txt\", \"kashiwabayashino_yoru.txt\",\n",
        "         \"kazeno_matasaburo.txt\", \"kiirono_tomato.txt\", \"oinomorito_zarumori.txt\"]  # 青空文庫より\n",
        "\n",
        "text = \"\"\n",
        "for novel in novels:\n",
        "    with open(\"kenji_novels/\"+novel, mode=\"r\", encoding=\"utf-8\") as f:  # ファイルの読み込み\n",
        "        text_novel = f.read()\n",
        "    text_novel = re.sub(\"《[^》]+》\", \"\", text_novel)  # ルビの削除\n",
        "    text_novel = re.sub(\"［[^］]+］\", \"\", text_novel)  # 読みの注意の削除\n",
        "    text_novel = re.sub(\"〔[^〕]+〕\", \"\", text_novel)  # 読みの注意の削除\n",
        "    text_novel = re.sub(\"[ 　\\n「」『』（）｜※＊…]\", \"\", text_novel)  # 全角半角スペース、改行、その他記号の削除\n",
        "    text += text_novel\n",
        "\n",
        "print(\"文字数:\", len(text))\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjjOJo_GJqCx"
      },
      "source": [
        "## 漢字をひらがなに変換\n",
        "漢字を平仮名に変換し、テキストをひらがな、カタカナ、記号のみにします。  \n",
        "文字数を絞ることで、モデルの精度を上げて学習時間を短くします。  \n",
        "漢字を平仮名に変換するためには、pykakasiというライブラリを使います。  \n",
        "pykakasiがうまく動作しない方は、前処理済みのテキストファイル（kana_kenji.txt）がありますので、それをご利用ください。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pykakasi"
      ],
      "metadata": {
        "id": "U5YjWm_RLI3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nk8lOs4QJqCy"
      },
      "outputs": [],
      "source": [
        "from pykakasi import kakasi\n",
        "\n",
        "seperator = \"。\"  # 。をセパレータに指定\n",
        "sentence_list = text.split(seperator)  # セパレーターを使って文章をリストに分割する\n",
        "sentence_list.pop() # 最後の要素は空の文字列になるので、削除\n",
        "sentence_list = [x+seperator for x in sentence_list]  # 文章の最後に。を追加\n",
        "\n",
        "kakasi = kakasi()\n",
        "kakasi.setMode(\"J\", \"H\")  # J(漢字) からH(ひらがな)へ\n",
        "conv = kakasi.getConverter()\n",
        "for sentence in sentence_list:\n",
        "    print(sentence)\n",
        "    print(conv.do(sentence))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTWEA4ZwJqC0"
      },
      "source": [
        "エラーが発生しました。  \n",
        "pykakasiの辞書に無い「苹」という文字が問題となっているようです。  \n",
        "この文字を予めひらがなに変換した上で、再び変換を行います。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ActWGnC4JqC2"
      },
      "outputs": [],
      "source": [
        "text = text.replace(\"苹果\", \"ひょうか\")\n",
        "\n",
        "seperator = \"。\"\n",
        "sentence_list = text.split(seperator) \n",
        "sentence_list.pop() \n",
        "sentence_list = [x+seperator for x in sentence_list]\n",
        "\n",
        "for sentence in sentence_list:\n",
        "    print(sentence)\n",
        "    print(conv.do(sentence))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htP8mrPAJqC5"
      },
      "source": [
        "エラーは発生していませんね。  \n",
        "問題なくひらがなに変換できたようです。  \n",
        "\n",
        "次に、set()を使って文字の重複を無くし、使用されている文字の一覧を表示してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5NE98tjtJqC6"
      },
      "outputs": [],
      "source": [
        "kana_text = conv.do(text)  # 全体をひらがなに変換\n",
        "print(set(kana_text))  # set()で文字の重複をなくす"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M65N6k48JqC9"
      },
      "source": [
        "ひらがなとカタカナ、一部の記号のみが残りました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qghlsC8mJqC-"
      },
      "source": [
        "## テキストデータの保存\n",
        "テキストデータを保存し、いつでも使えるようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "t471rgl8JqC_"
      },
      "outputs": [],
      "source": [
        "print(kana_text)\n",
        "with open(\"kana_kenji.txt\", mode=\"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(kana_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERhaD9w9JqDB"
      },
      "source": [
        "保存したテキストファイルを読み込み、保存できていることを確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-FefabolJqDB"
      },
      "outputs": [],
      "source": [
        "with open(\"kana_kenji.txt\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    kana_text = f.read()\n",
        "print(kana_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NnCdjccJqDD"
      },
      "source": [
        "問題なく保存できているようですね。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbwKMIjuJqDD"
      },
      "source": [
        "## 課題\n",
        "漢字からひらがなへの変換が、不自然である箇所をいくつか探してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XD4c0L2SJqDE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "dialogue_corpus.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}