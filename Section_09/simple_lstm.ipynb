{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicelindel3/nlp/blob/main/Section_09/simple_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6OAr0W8dh2S"
      },
      "source": [
        "# シンプルなLSTMの実装\n",
        "LSTM層を用いてシンプルなニューラルネットワークを構築し、時系列データを学習します。  \n",
        "今回は、通常のRNNとLSTMを比較します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K34zTF4rdh2b"
      },
      "source": [
        "## 訓練用データの作成\n",
        "訓練用のデータを作成します。  \n",
        "サイン関数に乱数でノイズを加えたデータを作成し、過去の時系列データから未来の値を予測できるようにします。  \n",
        "以前にシンプルなRNNを構築した際と異なり、今回は最後の時刻の出力のみ利用します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5F1oqzcRdh2d"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = np.linspace(-2*np.pi, 2*np.pi)  # -2πから2πまで\n",
        "sin_data = np.sin(x_data) + 0.1*np.random.randn(len(x_data))  # sin関数に乱数でノイズを加える\n",
        "\n",
        "plt.plot(x_data, sin_data)\n",
        "plt.show()\n",
        "\n",
        "n_rnn = 10  # 時系列の数\n",
        "n_sample = len(x_data)-n_rnn  # サンプル数\n",
        "x = np.zeros((n_sample, n_rnn))  # 入力\n",
        "t = np.zeros((n_sample,))  # 正解、最後の時刻のみ\n",
        "\n",
        "for i in range(0, n_sample):\n",
        "    x[i] = sin_data[i:i+n_rnn]\n",
        "    t[i] = sin_data[i+n_rnn]  # 入力の時系列より一つ後の値\n",
        "\n",
        "x = x.reshape(n_sample, n_rnn, 1)  # （サンプル数、時系列の数、入力層のニューロン数）\n",
        "print(x.shape)\n",
        "t = t.reshape(n_sample, 1)  # （サンプル数、入力層のニューロン数）\n",
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBIWO8hCdh2g"
      },
      "source": [
        "## SimpleRNNとLSTMの比較\n",
        "Kerasを使って通常のRNN、およびLSTMを構築します。  \n",
        "Kerasにおいて、LSTM層はSimpleRNN層と同じ方法で追加することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "DF_0yBgJdh2i"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SimpleRNN, LSTM\n",
        "\n",
        "batch_size = 8  # バッチサイズ\n",
        "n_in = 1  # 入力層のニューロン数\n",
        "n_mid = 20  # 中間層のニューロン数\n",
        "n_out = 1  # 出力層のニューロン数\n",
        "\n",
        "# 比較のための通常のRNN\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(SimpleRNN(n_mid, input_shape=(n_rnn, n_in), return_sequences=False))  # 最後の出力のみ使用\n",
        "model_rnn.add(Dense(n_out, activation=\"linear\"))\n",
        "model_rnn.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "print(model_rnn.summary())\n",
        "\n",
        "# LSTM\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(n_mid, input_shape=(n_rnn, n_in), return_sequences=False))\n",
        "model_lstm.add(Dense(n_out, activation=\"linear\"))\n",
        "model_lstm.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
        "print(model_lstm.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jucU5037dh2q"
      },
      "source": [
        "SimpleRNNよりも、LSTMの方がパラメータがずっと多いですね。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE7-OBYldh2r"
      },
      "source": [
        "## 学習\n",
        "構築したRNNのモデルを使って、学習を行います。  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "YHdT9MTmdh2t"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "# 通常のRNN\n",
        "start_time = time.time()\n",
        "history_rnn = model_rnn.fit(x, t, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "print(\"学習時間 --通常のRNN--:\", time.time() - start_time)\n",
        "\n",
        "# LSTM\n",
        "start_time = time.time()\n",
        "history_lstm = model_lstm.fit(x, t, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "print(\"学習時間 --LSTM--:\", time.time() - start_time)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaIPcynadh2u"
      },
      "source": [
        "エポック数が同じ場合、パラメータ数が多いためLSTMの方が学習にずっと時間がかかります。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k5A8YU2dh2v"
      },
      "source": [
        "## 学習の推移\n",
        "誤差の推移を確認します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ey9KT-uvdh2v"
      },
      "outputs": [],
      "source": [
        "loss_rnn = history_rnn.history['loss']\n",
        "loss_lstm = history_lstm.history['loss']\n",
        "\n",
        "plt.plot(np.arange(len(loss_rnn)), loss_rnn, label=\"RNN\")\n",
        "plt.plot(np.arange(len(loss_lstm)), loss_lstm, label=\"LSTM\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQc_of6qdh2w"
      },
      "source": [
        "モデルが複雑なため、LSTMの方が誤差の収束にエポック数が必要です。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g4MBwJgdh2x"
      },
      "source": [
        "## 学習済みモデルの使用\n",
        "それぞれの学習済みモデルを使って、サイン関数の次の値を予測します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Gy_BIsFzdh2x"
      },
      "outputs": [],
      "source": [
        "predicted_rnn = x[0].reshape(-1) \n",
        "predicted_lstm = x[0].reshape(-1) \n",
        "\n",
        "for i in range(0, n_sample):\n",
        "    y_rnn = model_rnn.predict(predicted_rnn[-n_rnn:].reshape(1, n_rnn, 1))\n",
        "    predicted_rnn = np.append(predicted_rnn, y_rnn[0][0])\n",
        "    y_lstm = model_lstm.predict(predicted_lstm[-n_rnn:].reshape(1, n_rnn, 1))\n",
        "    predicted_lstm = np.append(predicted_lstm, y_lstm[0][0])\n",
        "\n",
        "plt.plot(np.arange(len(sin_data)), sin_data, label=\"Training data\")\n",
        "plt.plot(np.arange(len(predicted_rnn)), predicted_rnn, label=\"Predicted_RNN\")\n",
        "plt.plot(np.arange(len(predicted_lstm)), predicted_lstm, label=\"Predicted_LSTM\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5g1RCicmdh2z"
      },
      "source": [
        "LSTMを使ったモデルが、サインカーブを学習できていることが分かります。  \n",
        "\n",
        "このように、LSTMはRNNと同様に時系列データの学習ができるのですが、パラメータ数が多くモデルが複雑なため学習に時間がかかります。  \n",
        "今回の例からはLSTMのメリットはあまり分かりませんが、文脈が大事な自然言語処理などで、LSTMはその真価を発揮します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yLYdbSmdh20"
      },
      "source": [
        "## 課題\n",
        "サイン関数以外の、様々な関数をLSTMに学習させてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PuzIYpNZdh21"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_data = np.linspace(-2*np.pi, 2*np.pi)  # -2πから2πまで\n",
        "sin_data = np.cos(x_data) + 0.1*np.random.randn(len(x_data))  # sin関数に乱数でノイズを加える -> cos関数に変換\n",
        "\n",
        "plt.plot(x_data, sin_data)\n",
        "plt.show()\n",
        "\n",
        "n_rnn = 10  # 時系列の数\n",
        "n_sample = len(x_data)-n_rnn  # サンプル数\n",
        "x = np.zeros((n_sample, n_rnn))  # 入力\n",
        "t = np.zeros((n_sample,))  # 正解、最後の時刻のみ\n",
        "\n",
        "for i in range(0, n_sample):\n",
        "    x[i] = sin_data[i:i+n_rnn]\n",
        "    t[i] = sin_data[i+n_rnn]  # 入力の時系列より一つ後の値\n",
        "\n",
        "x = x.reshape(n_sample, n_rnn, 1)  # （サンプル数、時系列の数、入力層のニューロン数）\n",
        "print(x.shape)\n",
        "t = t.reshape(n_sample, 1)  # （サンプル数、入力層のニューロン数）\n",
        "print(t.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "epochs = 200\n",
        "\n",
        "# 通常のRNN\n",
        "start_time = time.time()\n",
        "history_rnn = model_rnn.fit(x, t, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "print(\"学習時間 --通常のRNN--:\", time.time() - start_time)\n",
        "\n",
        "# LSTM\n",
        "start_time = time.time()\n",
        "history_lstm = model_lstm.fit(x, t, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "print(\"学習時間 --LSTM--:\", time.time() - start_time)"
      ],
      "metadata": {
        "id": "kSbUZvAfhrYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_rnn = history_rnn.history['loss']\n",
        "loss_lstm = history_lstm.history['loss']\n",
        "\n",
        "plt.plot(np.arange(len(loss_rnn)), loss_rnn, label=\"RNN\")\n",
        "plt.plot(np.arange(len(loss_lstm)), loss_lstm, label=\"LSTM\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wRlEDeJDh7XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_rnn = x[0].reshape(-1) \n",
        "predicted_lstm = x[0].reshape(-1) \n",
        "\n",
        "for i in range(0, n_sample):\n",
        "    y_rnn = model_rnn.predict(predicted_rnn[-n_rnn:].reshape(1, n_rnn, 1))\n",
        "    predicted_rnn = np.append(predicted_rnn, y_rnn[0][0])\n",
        "    y_lstm = model_lstm.predict(predicted_lstm[-n_rnn:].reshape(1, n_rnn, 1))\n",
        "    predicted_lstm = np.append(predicted_lstm, y_lstm[0][0])\n",
        "\n",
        "plt.plot(np.arange(len(sin_data)), sin_data, label=\"Training data\")\n",
        "plt.plot(np.arange(len(predicted_rnn)), predicted_rnn, label=\"Predicted_RNN\")\n",
        "plt.plot(np.arange(len(predicted_lstm)), predicted_lstm, label=\"Predicted_LSTM\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SS2e1imyicZv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "colab": {
      "name": "simple_lstm.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}